# Basic benchmark configuration for flex attention
# Usage: python flex_perf.py --config config_basic.yaml

# Core parameters
dynamic: false
calculate_bwd: false
dtype: "bfloat16"

# Shape parameters
b: [2, 8, 16]  # batch sizes
nh: ["16, 16", "16, 4"]  # [query_heads,key_value_heads]
s: [1024, 4096]  # sequence lengths
d: [64, 128]  # head dimensions

# Attention types to benchmark
mods: ["noop", "causal", "alibi", "sliding_window"]

# Backend and optimization
backend: []
max_autotune: false

# Decoding and cache settings
decoding: false
kv_size: null  # Use batch sizes instead

# Metrics and output
throughput: true  # Always calculate TBS and TFLOPs
show_speedups: false  # Show speedup calculations
save_path: null  # No CSV output
