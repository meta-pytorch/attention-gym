# Basic benchmark configuration for flex attention
# Usage: python flex_perf.py --config config_basic.yaml

# Core parameters
dynamic: false
calculate_bwd: false
dtype: "float16"

# # Shape parameters
# b: [2, 4]  # batch sizes
# nh: ["16, 16", "16, 4"]  # [query_heads,key_value_heads]
# s: [1024, 2048, 4096, 8192, 16384, 32768]  # sequence lengths
# d: [64, 128]  # head dimensions

b: [2, 4, 8]
nh: ["16, 16", "32, 32"]  # query_heads, key_value_heads
s: [2048, 4096, 8192]
d: [64, 128]
mods: ["noop",]

# Attention types to benchmark
# mods: ["noop", "causal", "alibi", "sliding_window", "document_mask"]

# Backend and optimization
backend: []
max_autotune: True

# Decoding and cache settings
decoding: false
kv_size: null  # Use batch sizes instead

# Metrics and output
throughput: true  # Always calculate TBS and TFLOPs
show_speedups: false  # Show speedup calculations
save_path: null  # No CSV output

# Uncomment to change
kernel_options:
  global:
    USE_TMA: True
    ROWS_GUARANTEED_SAFE: True
    BLOCKS_ARE_CONTIGUOUS: True
#   global:
#     BLOCK_M: 64
#     BLOCK_N: 128
#     num_warps: 8
#     num_stages: 2
