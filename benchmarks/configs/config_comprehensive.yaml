# Comprehensive benchmark configuration for flex attention
# Usage: python flex_perf.py --config config_comprehensive.yaml

# Core parameters
dynamic: false
calculate_bwd: true  # Include backward pass timing
dtype: "bfloat16"

# Shape parameters - larger sweep
b: [1, 2, 4, 8, 16, 32]  # batch sizes
nh: ["16,16", "16,2", "32,32", "32,4"]  # [query_heads,key_value_heads]
s: [512, 1024, 2048, 4096, 8192]  # sequence lengths
d: [64, 128, 256]  # head dimensions

# All attention types
mods: ["noop", "causal", "rel", "head_bias", "alibi", "sliding_window", "prefix_lm", "softcap"]

# Multiple backends for comparison
backend: ["efficient", "math", "fav2"]
max_autotune: true

# Decoding and cache settings
decoding: false
kv_size: null

# Metrics and output
throughput: true  # Always calculate TBS and TFLOPs
show_speedups: true  # Show speedup calculations
save_path: "comprehensive_results.csv"  # Save to CSV
